{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aad2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training import *\n",
    "# # # # # ALSO IMPORTS THE FOLLOWING\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from .models.voxnet import VoxNet\n",
    "# from functools import partial\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "#\n",
    "# and the train_one_epoch function\n",
    "\n",
    "from src.dataset import ModelNetDataset\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed887e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from .models.voxnet import VoxNet\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray import tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfed820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "        model,\n",
    "        training_loader,\n",
    "        optimizer, \n",
    "        loss_c_fn, # loss for class\n",
    "        loss_o_fn, # loss for orientation\n",
    "        gamma,     # relative weights of the losses\n",
    "        log_every : int = 10, # batches\n",
    "        device : str = 'cpu'\n",
    "                    ):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    ------\n",
    "        model : nn.Module to train. Expected to return a tuple (orientation,class) already one hot encoded.\n",
    "        training_loader : torch.utils.data.DataLoader pointing to the training subset\n",
    "        optimizer : torch.optim optimizer\n",
    "        loss_c_fn : torch.nn loss for the class output\n",
    "        loss_o_fn :               for the orientation output\n",
    "        gamma : float, relative weights for the losses\n",
    "        log_every : int, number of batches between logging to tune\n",
    "        device : str, device to cast the data to. must be the same as model\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        last_loss : the training loss on the last log_every subset of batches\n",
    "    \"\"\"\n",
    "\n",
    "    running_loss = 0\n",
    "    last_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(training_loader):\n",
    "        \n",
    "        # Every data instance is (voxel grid, o_y, y) already hot encoded\n",
    "        voxels, o_y, y = data\n",
    "\n",
    "        # cast them all to float and transfer to device\n",
    "        voxels = voxels.float().to(device)\n",
    "        o_y = o_y.float().to(device)\n",
    "        y = y.float().to(device)\n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        o_y_pred, y_pred = model(voxels)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss_c = loss_c_fn(y_pred, y)\n",
    "        loss_o = loss_o_fn(o_y_pred,o_y)\n",
    "\n",
    "        total_loss = (1-gamma)*loss_c + gamma*loss_o\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "        if i % log_every == log_every-1:  # print every 2000 mini-batches\n",
    "            last_loss = running_loss / log_every # return the loss over the last log_every batches\n",
    "            running_loss = 0.0 # reset the running_loss\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    training_set,\n",
    "    validation_set,\n",
    "    config,\n",
    "    num_workers : int = 1, # for dataloader processes\n",
    "    load_model_path = None,\n",
    "    n_epochs : int = 10,\n",
    "    log_every : int = 20\n",
    "    ):\n",
    "\n",
    "    \"\"\"\n",
    "    config must have \n",
    "\n",
    "    batch_size\n",
    "    lr\n",
    "    gamma\n",
    "    \"\"\"\n",
    "\n",
    "    # Create data loaders for our datasets; shuffle for training, not for validation\n",
    "    training_loader = DataLoader(training_set, batch_size=config['batch_size'], shuffle=True, num_workers=num_workers)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=config['batch_size'], shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # ONLY WORKS FOR NPY DATASETS\n",
    "    N_ORIENTATION_CLASSES=training_set.metadata.loc['npy']['orientation_class_id'].unique().size\n",
    "    N_CLASSES=training_set.metadata.loc['npy']['label_int'].unique().size\n",
    "\n",
    "    model = VoxNet()\n",
    "    if load_model_path is not None: \n",
    "        print('Loading state...')\n",
    "        print(load_model_path)\n",
    "        model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "    # device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    # LOSS FUNCTION\n",
    "\n",
    "    loss_c_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss_o_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "    # OPTIMIZER\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=config['lr'])\n",
    "\n",
    "    # # learning rate scheduler, update LR every epoch\n",
    "    # scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=GAMMA_LR)\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "\n",
    "        ######################## TRAINING\n",
    "\n",
    "        for i, data in enumerate(training_loader):\n",
    "            # Every data instance is (voxel grid, o_y, y) ordinal encoded\n",
    "            voxels, o_y, y = data\n",
    "            voxels = voxels.to(device)\n",
    "            # we need one hot encoding for cross entropy loss\n",
    "            o_y = F.one_hot(o_y,num_classes=N_ORIENTATION_CLASSES).float().to(device)\n",
    "            y = F.one_hot(y,num_classes=N_CLASSES).float().to(device)\n",
    "\n",
    "            # Zero your gradients for every batch!\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            y_pred, o_y_pred = model(voxels.float())\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss_c = loss_c_fn(y_pred, y)\n",
    "            loss_o = loss_o_fn(o_y_pred,o_y)\n",
    "\n",
    "            total_loss = (1-config['gamma'])*loss_c + config['gamma']*loss_o\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += total_loss.item()\n",
    "            epoch_steps += 1\n",
    "            if i % log_every == log_every-1:  # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
    "                                                running_loss / epoch_steps))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "\n",
    "        \n",
    "        ######################## VALIDATION\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, v_data in enumerate(validation_loader):\n",
    "                # get prediction and target\n",
    "                v_voxels,v_o_y,v_y = v_data\n",
    "                v_voxels = v_voxels.to(device)\n",
    "                v_y_pred, v_o_y_pred = model(v_voxels.float())\n",
    "                v_y_pred, v_o_y_pred = v_y_pred.to(device), v_o_y_pred.to(device)\n",
    "\n",
    "                # one hot encoding of y from dataset\n",
    "                v_o_y = F.one_hot(v_o_y,num_classes=N_ORIENTATION_CLASSES).float().to(device)\n",
    "                v_y = F.one_hot(v_y,num_classes=N_CLASSES).float().to(device)\n",
    "\n",
    "                # compute loss\n",
    "                loss_c = loss_c_fn(v_y_pred, v_y)\n",
    "                loss_o = loss_o_fn(v_o_y_pred,v_o_y)\n",
    "                vtotal_loss = (1-config['gamma'])*loss_c + config['gamma']*loss_o\n",
    "\n",
    "                # extract prediction from NN\n",
    "                true_orientation=v_o_y.argmax(1)\n",
    "                predicted_orientation=v_o_y_pred.argmax(1)\n",
    "                correct_orientation_prediction = true_orientation == predicted_orientation\n",
    "\n",
    "                true_label=v_y.argmax(1)\n",
    "                predicted_label=v_y_pred.argmax(1)\n",
    "                correct_label_prediction = true_label == predicted_label\n",
    "\n",
    "                # HOW TO COMPUTE ACCURACY? \n",
    "                # consider correct only those with correct label AND orientation\n",
    "\n",
    "                correct_combined = correct_label_prediction*correct_orientation_prediction\n",
    "                accuracy = correct_combined.sum() / correct_combined.size()\n",
    "\n",
    "\n",
    "\n",
    "                val_loss += vtotal_loss.numpy()\n",
    "                val_steps+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ################ CHECKPOINT\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        tuner = tune.Tuner(train, param_space=trial_space)\n",
    "        results = tuner.fit()\n",
    "        reults.report(loss=(val_loss / val_steps), accuracy=accuracy)\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974f7470",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_161423/632129570.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrial_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_loader' is not defined"
     ]
    }
   ],
   "source": [
    "trial_space = {\n",
    "    \"scaling_config\": ScalingConfig(num_workers=tune.grid_search([1,2,3])),\n",
    "    \"batch_size\": [200,300,400,500,600],\n",
    "    \"lr\": np.linspace(0.05,0.85,10),\n",
    "    \"gamma\": np.linspace(0.1,1,10)\n",
    "    \n",
    "}\n",
    "results = train(training_loader, validation_loader,trial_space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf76ce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_161423/4207269895.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_dataframe\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "dfs = {results.log_dir: results.metrics_dataframe for result in results}\n",
    "[d.mean_accuracy.plot() for d in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97723c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
